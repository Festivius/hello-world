{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Festivius/hello-world/blob/main/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UTjFft5rLoJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gbsjL1trPSP"
      },
      "outputs": [],
      "source": [
        "transform = transforms.ToTensor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMo416LbrYoC"
      },
      "outputs": [],
      "source": [
        "train_data = datasets.MNIST(root='/cnn_data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='/cnn_data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "550Ech9srcos"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ-K5NpOrgCr"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(196, 33)\n",
        "    self.fc2 = nn.Linear(33, 25)\n",
        "    self.fc3 = nn.Linear(25, 10)\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "    X = X.view(-1, 196)\n",
        "\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = self.fc3(X)\n",
        "\n",
        "    return F.log_softmax(X, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4VeEazmrtrX"
      },
      "outputs": [],
      "source": [
        "model = ConvolutionalNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOwp5Rf2rutP"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWmvBGMbrvpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca48b6e-fb4f-4f2a-9719-3e25a23d2f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0  Batch: 1000  Loss: 0.28416404128074646\n",
            "Epoch: 0  Batch: 2000  Loss: 0.2713087201118469\n",
            "Epoch: 0  Batch: 3000  Loss: 0.2503032684326172\n",
            "Epoch: 0  Batch: 4000  Loss: 0.2004493921995163\n",
            "Epoch: 0  Batch: 5000  Loss: 0.02620089054107666\n",
            "Epoch: 0  Batch: 6000  Loss: 0.061026811599731445\n",
            "Epoch: 1  Batch: 1000  Loss: 0.29720428586006165\n",
            "Epoch: 1  Batch: 2000  Loss: 0.09474434703588486\n",
            "Epoch: 1  Batch: 3000  Loss: 0.5805155038833618\n",
            "Epoch: 1  Batch: 4000  Loss: 0.05504431203007698\n",
            "Epoch: 1  Batch: 5000  Loss: 0.021260861307382584\n",
            "Epoch: 1  Batch: 6000  Loss: 0.37327665090560913\n",
            "Epoch: 2  Batch: 1000  Loss: 0.025350576266646385\n",
            "Epoch: 2  Batch: 2000  Loss: 0.20941917598247528\n",
            "Epoch: 2  Batch: 3000  Loss: 0.024845723062753677\n",
            "Epoch: 2  Batch: 4000  Loss: 0.028816912323236465\n",
            "Epoch: 2  Batch: 5000  Loss: 0.037517525255680084\n",
            "Epoch: 2  Batch: 6000  Loss: 0.05883635953068733\n",
            "Epoch: 3  Batch: 1000  Loss: 0.0488986074924469\n",
            "Epoch: 3  Batch: 2000  Loss: 0.040872085839509964\n",
            "Epoch: 3  Batch: 3000  Loss: 0.008130254223942757\n",
            "Epoch: 3  Batch: 4000  Loss: 0.18653807044029236\n",
            "Epoch: 3  Batch: 5000  Loss: 0.0921531617641449\n",
            "Epoch: 3  Batch: 6000  Loss: 0.004244911950081587\n",
            "Epoch: 4  Batch: 1000  Loss: 0.014850052073597908\n",
            "Epoch: 4  Batch: 2000  Loss: 0.034606412053108215\n",
            "Epoch: 4  Batch: 3000  Loss: 0.47447723150253296\n",
            "Epoch: 4  Batch: 4000  Loss: 0.19315126538276672\n",
            "Epoch: 4  Batch: 5000  Loss: 0.013999180868268013\n",
            "Epoch: 4  Batch: 6000  Loss: 0.003345468547195196\n",
            "Epoch: 5  Batch: 1000  Loss: 0.09872359782457352\n",
            "Epoch: 5  Batch: 2000  Loss: 0.024697616696357727\n",
            "Epoch: 5  Batch: 3000  Loss: 0.04728323966264725\n",
            "Epoch: 5  Batch: 4000  Loss: 0.13330744206905365\n",
            "Epoch: 5  Batch: 5000  Loss: 0.02673419751226902\n",
            "Epoch: 5  Batch: 6000  Loss: 0.014708313159644604\n",
            "Epoch: 6  Batch: 1000  Loss: 0.005982609000056982\n",
            "Epoch: 6  Batch: 2000  Loss: 0.018460456281900406\n",
            "Epoch: 6  Batch: 3000  Loss: 0.010826329700648785\n",
            "Epoch: 6  Batch: 4000  Loss: 0.001996388426050544\n",
            "Epoch: 6  Batch: 5000  Loss: 0.025181716307997704\n",
            "Epoch: 6  Batch: 6000  Loss: 0.05590589717030525\n",
            "Epoch: 7  Batch: 1000  Loss: 0.15339845418930054\n",
            "Epoch: 7  Batch: 2000  Loss: 0.008089091628789902\n",
            "Epoch: 7  Batch: 3000  Loss: 0.1193166971206665\n",
            "Epoch: 7  Batch: 4000  Loss: 0.05171287804841995\n",
            "Epoch: 7  Batch: 5000  Loss: 0.30850496888160706\n",
            "Epoch: 7  Batch: 6000  Loss: 0.002218171488493681\n",
            "Epoch: 8  Batch: 1000  Loss: 0.014764642342925072\n",
            "Epoch: 8  Batch: 2000  Loss: 0.007073849439620972\n",
            "Epoch: 8  Batch: 3000  Loss: 0.23795798420906067\n",
            "Epoch: 8  Batch: 4000  Loss: 0.026585523039102554\n",
            "Epoch: 8  Batch: 5000  Loss: 0.04604353383183479\n",
            "Epoch: 8  Batch: 6000  Loss: 0.0017985660815611482\n",
            "Epoch: 9  Batch: 1000  Loss: 0.0026522905100136995\n",
            "Epoch: 9  Batch: 2000  Loss: 0.20309977233409882\n",
            "Epoch: 9  Batch: 3000  Loss: 0.01822928711771965\n",
            "Epoch: 9  Batch: 4000  Loss: 0.049349844455718994\n",
            "Epoch: 9  Batch: 5000  Loss: 0.03623941168189049\n",
            "Epoch: 9  Batch: 6000  Loss: 0.009145641699433327\n",
            "Epoch: 10  Batch: 1000  Loss: 0.021573785692453384\n",
            "Epoch: 10  Batch: 2000  Loss: 0.11062945425510406\n",
            "Epoch: 10  Batch: 3000  Loss: 0.30073076486587524\n",
            "Epoch: 10  Batch: 4000  Loss: 0.01519728172570467\n",
            "Epoch: 10  Batch: 5000  Loss: 0.0018198273610323668\n",
            "Epoch: 10  Batch: 6000  Loss: 0.014819910749793053\n",
            "Epoch: 11  Batch: 1000  Loss: 0.008207570761442184\n",
            "Epoch: 11  Batch: 2000  Loss: 0.012484358623623848\n",
            "Epoch: 11  Batch: 3000  Loss: 0.002475283108651638\n",
            "Epoch: 11  Batch: 4000  Loss: 0.09489528834819794\n",
            "Epoch: 11  Batch: 5000  Loss: 0.04726801812648773\n",
            "Epoch: 11  Batch: 6000  Loss: 0.12168645858764648\n",
            "Epoch: 12  Batch: 1000  Loss: 0.016527550294995308\n",
            "Epoch: 12  Batch: 2000  Loss: 0.023255739361047745\n",
            "Epoch: 12  Batch: 3000  Loss: 0.009907374158501625\n",
            "Epoch: 12  Batch: 4000  Loss: 0.33144277334213257\n",
            "Epoch: 12  Batch: 5000  Loss: 0.5858794450759888\n",
            "Epoch: 12  Batch: 6000  Loss: 0.1228099837899208\n",
            "Epoch: 13  Batch: 1000  Loss: 0.030002838000655174\n",
            "Epoch: 13  Batch: 2000  Loss: 0.20754781365394592\n",
            "Epoch: 13  Batch: 3000  Loss: 0.0021224678494036198\n",
            "Epoch: 13  Batch: 4000  Loss: 0.0024160733446478844\n",
            "Epoch: 13  Batch: 5000  Loss: 0.003423995804041624\n",
            "Epoch: 13  Batch: 6000  Loss: 0.09845640510320663\n",
            "Epoch: 14  Batch: 1000  Loss: 0.013389033265411854\n",
            "Epoch: 14  Batch: 2000  Loss: 0.023503068834543228\n",
            "Epoch: 14  Batch: 3000  Loss: 0.0018960169982165098\n",
            "Epoch: 14  Batch: 4000  Loss: 0.015825409442186356\n",
            "Epoch: 14  Batch: 5000  Loss: 0.0006643300876021385\n",
            "Epoch: 14  Batch: 6000  Loss: 0.030154433101415634\n",
            "Epoch: 15  Batch: 1000  Loss: 0.0008576976251788437\n",
            "Epoch: 15  Batch: 2000  Loss: 0.005831541959196329\n",
            "Epoch: 15  Batch: 3000  Loss: 0.00010939511412288994\n",
            "Epoch: 15  Batch: 4000  Loss: 0.0840313509106636\n",
            "Epoch: 15  Batch: 5000  Loss: 0.020775053650140762\n",
            "Epoch: 15  Batch: 6000  Loss: 0.004417030606418848\n",
            "Epoch: 16  Batch: 1000  Loss: 0.1057673841714859\n",
            "Epoch: 16  Batch: 2000  Loss: 0.06616455316543579\n",
            "Epoch: 16  Batch: 3000  Loss: 0.0003189006238244474\n",
            "Epoch: 16  Batch: 4000  Loss: 0.01409254502505064\n",
            "Epoch: 16  Batch: 5000  Loss: 0.0021038430277258158\n",
            "Epoch: 16  Batch: 6000  Loss: 0.04099983721971512\n",
            "Epoch: 17  Batch: 1000  Loss: 0.04222623631358147\n",
            "Epoch: 17  Batch: 2000  Loss: 0.0058340029790997505\n",
            "Epoch: 17  Batch: 3000  Loss: 0.045770976692438126\n",
            "Epoch: 17  Batch: 4000  Loss: 0.00051674316637218\n",
            "Epoch: 17  Batch: 5000  Loss: 0.05774422362446785\n",
            "Epoch: 17  Batch: 6000  Loss: 0.08287253975868225\n",
            "Epoch: 18  Batch: 1000  Loss: 0.09959524869918823\n",
            "Epoch: 18  Batch: 2000  Loss: 0.01172893587499857\n",
            "Epoch: 18  Batch: 3000  Loss: 0.0022056864108890295\n",
            "Epoch: 18  Batch: 4000  Loss: 0.004850364290177822\n",
            "Epoch: 18  Batch: 5000  Loss: 0.007224711123853922\n",
            "Epoch: 18  Batch: 6000  Loss: 0.14050492644309998\n",
            "Epoch: 19  Batch: 1000  Loss: 0.24753646552562714\n",
            "Epoch: 19  Batch: 2000  Loss: 0.0015437217662110925\n",
            "Epoch: 19  Batch: 3000  Loss: 0.4349450170993805\n",
            "Epoch: 19  Batch: 4000  Loss: 0.005070121958851814\n",
            "Epoch: 19  Batch: 5000  Loss: 0.0033173230476677418\n",
            "Epoch: 19  Batch: 6000  Loss: 0.0004867807438131422\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "\n",
        "for i in range(epochs):\n",
        "  for b,(X_train, y_train) in enumerate(train_loader):\n",
        "    b += 1\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    predicted = torch.max(y_pred.data, 1)[1]\n",
        "    batch_corr = (predicted == y_train).sum()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if b%1000 == 0:\n",
        "      print(f\"Epoch: {i}  Batch: {b}  Loss: {loss.item()}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for b,(X_test, y_test) in enumerate(test_loader):\n",
        "      y_val = model(X_test)\n",
        "      predicted = torch.max(y_val.data, 1)[1]\n",
        "  loss = criterion(y_val, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_load_everything = DataLoader(test_data, batch_size=10000, shuffle=False)"
      ],
      "metadata": {
        "id": "_5wM9s5rwwuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g1-M7NPr5jf"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  for X_test, y_test in test_load_everything:\n",
        "    y_val = model(X_test)\n",
        "    predicted = torch.max(y_val, 1)[1]\n",
        "    correct += (predicted == y_test).sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHZw27qFwzc4",
        "outputId": "c96e56f6-9009-486e-8ed1-8831f5eba3d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9597"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_fc1 = model.fc1.weight.data.tolist()\n",
        "weights_fc2 = model.fc2.weight.data.tolist()\n",
        "weights_fc3 = model.fc3.weight.data.tolist()\n",
        "\n",
        "for i in range(33):\n",
        "  for j in range(196):\n",
        "    weights_fc1[i][j] = round(weights_fc1[i][j], 4)\n",
        "\n",
        "for i in range(25):\n",
        "  for j in range(33):\n",
        "    weights_fc2[i][j] = round(weights_fc2[i][j], 4)\n",
        "\n",
        "for i in range(10):\n",
        "  for j in range(25):\n",
        "    weights_fc3[i][j] = round(weights_fc3[i][j], 4)\n",
        "\n",
        "with open('weights_fc1.json', 'w') as f:\n",
        "  json.dump(weights_fc1, f)\n",
        "\n",
        "with open('weights_fc2.json', 'w') as f:\n",
        "  json.dump(weights_fc2, f)\n",
        "\n",
        "with open('weights_fc3.json', 'w') as f:\n",
        "  json.dump(weights_fc3, f)"
      ],
      "metadata": {
        "id": "II_wA2oyWbtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biases_fc1 = model.fc1.bias\n",
        "biases_fc2 = model.fc2.bias\n",
        "biases_fc3 = model.fc3.bias"
      ],
      "metadata": {
        "id": "EDPNvuzdapGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biases_fc3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz4FSRV6auvN",
        "outputId": "794dbb2f-1c9e-4e41-c595-ed0aa542e993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.2921,  0.1034,  0.1809, -0.3112,  0.4765,  0.4518, -0.3592, -0.1181,\n",
              "         0.1247, -0.1593], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMU8z9TZQYBt17qti2VY7V0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}